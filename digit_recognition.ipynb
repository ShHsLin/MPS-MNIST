{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "digit_recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X672_w-dK20C",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1632912545355,
     "user_tz": -120,
     "elapsed": 9323,
     "user": {
      "displayName": "Olivier Kuijpers",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10927088579494083404"
     }
    },
    "outputId": "2c3011a4-bf61-4045-e6c9-0e080f7117c6"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jf-1KMMJK20G",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1632912549757,
     "user_tz": -120,
     "elapsed": 1433,
     "user": {
      "displayName": "Olivier Kuijpers",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10927088579494083404"
     }
    },
    "outputId": "5c92a64d-9a18-48c3-c0d1-7e3fe6ce55de"
   },
   "source": [
    "cd /content/drive/Othercomputers/My Laptop/Documents/TUM/Courses/mps_mnist"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/Othercomputers/My Laptop/Documents/TUM/Courses/mps_mnist\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6HMZiBllK20H",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1632912555367,
     "user_tz": -120,
     "elapsed": 248,
     "user": {
      "displayName": "Olivier Kuijpers",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10927088579494083404"
     }
    }
   },
   "source": [
    "\n",
    "import sys\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse.linalg\n",
    "from numba import jit\n",
    "from tqdm import notebook\n",
    "import pandas as pd\n",
    "\n",
    "from data_loader import *\n",
    "import os\n",
    "\n",
    "plt.style.use('report_stylesheet_standard.mplstyle')\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    def inner(*args, **kwargs):\n",
    "        t0 = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        print('duration of \"{}\": {}'.format(func.__name__, time.time() - t0))\n",
    "        return result\n",
    "\n",
    "    return inner\n",
    "\n",
    "\n",
    "def save_data_to_pickle(func, arguments, file_path, run_always=False):\n",
    "    if os.path.exists(file_path) and not run_always:\n",
    "\n",
    "        with open(file_path, 'rb') as f:\n",
    "            print('Data already exists, loading data...')\n",
    "            result = pickle.load(f)\n",
    "\n",
    "    else:\n",
    "        if not os.path.exists(os.path.dirname(file_path)):\n",
    "            print(os.path.dirname(file_path))\n",
    "            os.makedirs(os.path.dirname(file_path))\n",
    "        result = func(*arguments)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(result, f)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# @timeit\n",
    "# def overlap(bra, ket=None):\n",
    "#\n",
    "#     \"\"\"\n",
    "#     More efficient:\n",
    "#     1) ket to left\n",
    "#     2) (ket and left) to bra\n",
    "#     :param bra:\n",
    "#     :param ket:\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#\n",
    "#     if ket is None:\n",
    "#         ket = bra\n",
    "#\n",
    "#     braket = np.ones((1, 1))\n",
    "#     for bra_i, ket_i in zip(bra, ket):\n",
    "#         ket_i = np.tensordot(braket, ket_i, (1, 0))\n",
    "#         braket = np.tensordot(bra_i.conj(), ket_i, ((0, 1), (0, 1)))\n",
    "#     return braket.item()\n"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "gKRSRuiAK20I",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1632912562296,
     "user_tz": -120,
     "elapsed": 317,
     "user": {
      "displayName": "Olivier Kuijpers",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10927088579494083404"
     }
    }
   },
   "source": [
    "class DataProcessing:\n",
    "    def __init__(self, dataset, mapping_basis, even_distribution=False, max_nb_images=None):\n",
    "        assert mapping_basis in ['orthogonal', 'original', 'sin_cos', 'sines', 'sines2'], 'mapping_basis should be one of [\"orthogonal\", \"original\", \"sin_cos\"]'\n",
    "        self.mapping_basis = mapping_basis\n",
    "        self.max_nb_images = max_nb_images\n",
    "        self.original_images, self.labels = dataset\n",
    "\n",
    "        self.images = self.transform(self.original_images)\n",
    "\n",
    "        if even_distribution:\n",
    "            self.sorted_images = self.get_digits(self.images, self.labels)\n",
    "            self.sorted_images = self.create_even_distribution(self.sorted_images)\n",
    "\n",
    "    @staticmethod\n",
    "    def flat_to_2d(batch):\n",
    "        H = W = int(np.sqrt(batch.shape[-1]))\n",
    "        return batch.reshape((-1, H, W))\n",
    "\n",
    "    @staticmethod\n",
    "    def square_to_flat(batch):\n",
    "        H = batch.shape[-1]\n",
    "        return batch.reshape((-1, H ** 2))\n",
    "\n",
    "    def mean_pooling(self, images_, filter_size):\n",
    "        images_2d = self.flat_to_2d(images_)\n",
    "        H, W = images_2d.shape[-2:]\n",
    "        H_new = H // filter_size\n",
    "        W_new = W // filter_size\n",
    "        new_shape = images_2d.shape[:1] + (H_new, filter_size, W_new, filter_size)\n",
    "        result = np.mean(images_2d.reshape(new_shape), axis=(2, 4))\n",
    "        result = self.square_to_flat(result)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def scale(images_, domain=(0, 255), range_=(0, 1)):\n",
    "        scaled = images_ / (domain[1] - domain[0]) * (range_[1] - range_[0]) + range_[0]\n",
    "        return scaled\n",
    "\n",
    "    def mapping(self, mat):\n",
    "        if self.mapping_basis == 'orthogonal':\n",
    "            qstate = np.array([np.exp(mat * 3.j * np.pi / 2) * np.cos(mat * np.pi / 2), np.exp(-mat * 3.j * np.pi / 2) * np.sin(mat * np.pi / 2)])\n",
    "        elif self.mapping_basis == 'original':\n",
    "            qstate = np.array([np.cos(mat * np.pi / 2), np.sin(mat * np.pi / 2)])\n",
    "        elif self.mapping_basis == 'sin_cos':\n",
    "            qstate = np.array([np.sin(np.pi*1/2 * mat),\n",
    "                               np.cos(np.pi*1/2 * mat),\n",
    "                               np.sin(np.pi*3/2 * mat),\n",
    "                               np.cos(np.pi*3/2 * mat)])\n",
    "\n",
    "        elif self.mapping_basis == 'sines':\n",
    "            qstate = np.array([np.sin(np.pi*1/2 * mat),\n",
    "                               np.sin(np.pi*3/2 * mat),\n",
    "                               np.sin(np.pi*5/2 * mat),\n",
    "                               np.sin(np.pi*7/2 * mat)])\n",
    "            qstate /= np.linalg.norm(qstate, axis=0)\n",
    "\n",
    "        elif self.mapping_basis == 'sines2':\n",
    "            qstate = np.array([np.sin(np.pi*1 * mat),\n",
    "                               np.sin(np.pi*2 * mat),\n",
    "                               np.sin(np.pi*3 * mat),\n",
    "                               np.sin(np.pi*4 * mat)])\n",
    "\n",
    "            # print(np.linalg.norm(qstate, axis=0).shape)\n",
    "            qstate /= (np.linalg.norm(qstate, axis=0))\n",
    "        else:\n",
    "            print('Mapping basis not valid')\n",
    "            return\n",
    "        return qstate.transpose(1, 2, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_digits(mat, labels_):\n",
    "        sorted_digits_ = []\n",
    "        for label in np.unique(labels_):\n",
    "            label_slice = (labels_ == label)\n",
    "            sorted_digits_.append(mat[label_slice])\n",
    "\n",
    "        return sorted_digits_\n",
    "\n",
    "    def transform(self, images_):\n",
    "        course_grain = self.mean_pooling(images_, 2)\n",
    "\n",
    "        if self.mapping_basis == 'sines':\n",
    "            scaled = self.scale(course_grain, domain=(0, 1), range_=(0.1,1))\n",
    "        elif self.mapping_basis == 'sines2':\n",
    "            scaled = self.scale(course_grain, domain=(0, 1), range_=(0.1,0.9))\n",
    "\n",
    "        else:\n",
    "            scaled = self.scale(course_grain, domain=(0, 1))\n",
    "        mapped = self.mapping(scaled)\n",
    "        return mapped\n",
    "\n",
    "    def create_even_distribution(self, batch):\n",
    "        if self.max_nb_images is None:\n",
    "            smallest_batch_size = np.min([len(dbatch) for dbatch in batch])\n",
    "            even_batch = np.array([dbatch[:smallest_batch_size] for dbatch in batch]).copy()\n",
    "            return even_batch\n",
    "        else:\n",
    "            even_batch = np.array([dbatch[:self.max_nb_images] for dbatch in batch]).copy()\n",
    "            return even_batch\n",
    "\n",
    "\n",
    "    def show_pic(self):\n",
    "        pixels = self.sorted_images.shape[-2]\n",
    "        h = int(pixels**(1/2))\n",
    "\n",
    "        for i in range(4):\n",
    "            plt.figure()\n",
    "            image  = self.sorted_images[0,0,:,i].reshape((h,h))\n",
    "            plt.imshow(image)\n"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "o-YMSItkK20K",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1632912569646,
     "user_tz": -120,
     "elapsed": 884,
     "user": {
      "displayName": "Olivier Kuijpers",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10927088579494083404"
     }
    }
   },
   "source": [
    "class Normalization:\n",
    "    def __init__(self, batch, mapping_basis):\n",
    "        nb_images = batch[0].shape[0]\n",
    "        norms_save_path = os.path.join('results',mapping_basis,'norms','norm_{}.p'.format(nb_images))\n",
    "        self.norms = save_data_to_pickle(self.get_norms_all_digits, (batch,), norms_save_path)\n",
    "        self.wfs = self.normalize_all_digits(batch, self.norms)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_all_digits(batch, norms):\n",
    "        for i in range(len(batch)):\n",
    "\n",
    "            batch[i] /= (norms[i] ** (1 / (2 * batch[i].shape[1])))\n",
    "        return batch\n",
    "\n",
    "    def get_norms_all_digits(self, batch):\n",
    "        norms = []\n",
    "        for digit_batch in notebook.tqdm_notebook(batch):\n",
    "            norm = self.get_norm_single_digit(digit_batch)\n",
    "            norms.append(norm)\n",
    "        return np.array(norms)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_norm_single_digit(digit_batch):\n",
    "        @jit(nopython=True)\n",
    "        def contract_physical(node_idx):\n",
    "            return np.ascontiguousarray(digit_batch[:, node_idx, :]) @ np.ascontiguousarray(\n",
    "                digit_batch[:, node_idx, :].T.conj())\n",
    "\n",
    "\n",
    "        product = contract_physical(0)\n",
    "        for i in range(1, 196):\n",
    "            product *= contract_physical(i)\n",
    "        norm = np.sum(product)\n",
    "        norm = np.abs(norm)\n",
    "        print(norm)\n",
    "        return norm\n",
    "\n",
    "    @staticmethod\n",
    "    def get_overlaps_single_image(image_mps, dfunc):\n",
    "        physical_contract = np.sum([image_mps[:,i].conj() * dfunc[:,:,:,i] for i in range(image_mps.shape[-1])], axis=0)\n",
    "        virtual_contract = np.prod(physical_contract, 2)\n",
    "        overlap = np.sum(virtual_contract, 1)\n",
    "        overlap = np.abs(overlap)\n",
    "        return overlap\n",
    "\n",
    "    @timeit\n",
    "    def get_accuracy_exact(self, test_images, test_labels, N=None):\n",
    "        if N is None:\n",
    "            N = len(test_images)\n",
    "        preds = []\n",
    "        for image in notebook.tqdm_notebook(test_images[:N]):\n",
    "            pred = np.argmax(self.get_overlaps_single_image(image, self.wfs))\n",
    "            preds.append(pred)\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        print(preds.shape)\n",
    "        accuracy = np.sum(test_labels[:N] == preds) / N\n",
    "\n",
    "        return accuracy\n"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "pQFUynCWK20L"
   },
   "source": [
    "class CompressedWFS:\n",
    "    def __init__(self, max_bond_dim, ewfs, mapping_basis):\n",
    "        self.mapping_basis = mapping_basis\n",
    "        self.ewfs = ewfs\n",
    "\n",
    "\n",
    "        self.nb_basis_elements = self.ewfs[0].shape[-1]\n",
    "\n",
    "        self.max_bond_dim = max_bond_dim\n",
    "        self.cwfs = self.create_random_compressed_wfs(196, max_bond_dim, self.nb_basis_elements)\n",
    "\n",
    "        self.cwfs_reshaped = self.reshape_cwfs()\n",
    "\n",
    "    def reshape_cwfs(self):\n",
    "        cwfs_reshaped = []\n",
    "        for m_i in range(len(self.cwfs[0])):\n",
    "            single_m = []\n",
    "            for mps in self.cwfs:\n",
    "                single_m.append(mps[m_i])\n",
    "\n",
    "            single_m = np.array(single_m)\n",
    "            cwfs_reshaped.append(single_m)\n",
    "            print(cwfs_reshaped[0].shape)\n",
    "        return cwfs_reshaped\n",
    "\n",
    "    def sweep(self, run_always=False, sweep_number=1):\n",
    "        def sweep_inner(self):\n",
    "            cwfs = []\n",
    "            print('Sweeping through MPS')\n",
    "            for cmps, emps in notebook.tqdm_notebook(zip(self.cwfs, self.ewfs)):\n",
    "                self.perform_svd_full_mps(cmps)\n",
    "                self.sweep_mps(cmps, emps)\n",
    "                cwfs.append(cmps)\n",
    "            return cwfs\n",
    "        cwfs_file_name = 'cwfs_chi{}_s{}.p'.format(self.max_bond_dim, sweep_number)\n",
    "        print('aaaa')\n",
    "        cwfs_path = os.path.join('results', self.mapping_basis ,'cwfs', cwfs_file_name)\n",
    "        self.cwfs = save_data_to_pickle(sweep_inner, (self,),\n",
    "                                        cwfs_path,\n",
    "                                        run_always=run_always)\n",
    "        self.cwfs_reshaped = self.reshape_cwfs()\n",
    "\n",
    "    @staticmethod\n",
    "    def sweep_mps(cmps, emps):\n",
    "        \"\"\"\n",
    "\n",
    "        :param cmps: list: 196 entries with shape [chi_left, physical, chi_right]\n",
    "        :param emps: np.array: [4506, 196, 2]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        nb_basis_elements = emps.shape[-1]\n",
    "\n",
    "        def contract_single_bond(i):\n",
    "            physical_contract_ = np.tensordot(emps[:, i, :], cmps[i].conj(), (1, 1))\n",
    "            return physical_contract_\n",
    "\n",
    "\n",
    "        right_contr_list = [np.ones((1,1,len(emps)))]\n",
    "\n",
    "\n",
    "        for m_idx in reversed(range(len(cmps) - 1)):\n",
    "            virtual = np.array([right_contr_list[-1] * emps[:,m_idx+1, basis_element] for basis_element in range(nb_basis_elements)])\n",
    "            # virtual = np.array([right_contr_list[-1] * emps[:,m_idx+1,0], right_contr_list[-1] * emps[:,m_idx+1,1]])\n",
    "            physical = np.tensordot(cmps[m_idx+1].conj(), virtual, ((1,2),(0,1)))\n",
    "            # physical_contract = contract_single_bond(m_idx + 1)\n",
    "            # right_contract = np.ascontiguousarray(physical_contract) @ np.ascontiguousarray(right_contr_list[-1])\n",
    "            right_contr_list.append(physical)\n",
    "        right_contr_list.reverse()\n",
    "\n",
    "\n",
    "\n",
    "        left_contr_list = [np.ones((1,1,len(emps)))]\n",
    "        for m_idx in range(len(cmps) - 1):\n",
    "            right_contract = right_contr_list[m_idx]\n",
    "            updated_m_left = np.array([left_contr_list[-1] * emps[:, m_idx, basis_element] for basis_element in range(nb_basis_elements)])\n",
    "            # updated_m_left = np.array([left_contr_list[-1] * emps[:, m_idx, 0], left_contr_list[-1] * emps[:, m_idx, 1]])\n",
    "            updated_m = np.tensordot(updated_m_left, right_contract, (-1, -1))\n",
    "\n",
    "            updated_m_sliced = updated_m[:, :, 0, :, 0].transpose((1, 0, 2))\n",
    "            legs_together = updated_m_sliced.reshape((-1, updated_m_sliced.shape[-1]))\n",
    "            A, S, V = scipy.linalg.svd(legs_together, full_matrices=False, lapack_driver='gesdd')\n",
    "\n",
    "            updated_m_left_canonical = A.reshape(updated_m_sliced.shape[0], nb_basis_elements, -1)\n",
    "            cmps[m_idx] = updated_m_left_canonical\n",
    "\n",
    "            virtual = np.array([left_contr_list[-1] * emps[:,m_idx, basis_element] for basis_element in range(nb_basis_elements)])\n",
    "            # virtual = np.array([left_contr_list[-1] * emps[:,m_idx,0], left_contr_list[-1] * emps[:,m_idx,1]])\n",
    "            physical = np.tensordot(cmps[m_idx].conj(), virtual, ((1,0),(0,1)))\n",
    "\n",
    "            # physical_contract = contract_single_bond(m_idx)\n",
    "            # contract_left = np.ascontiguousarray(left_contr_list[-1]) @ np.ascontiguousarray(physical_contract)\n",
    "            left_contr_list.append(physical)\n",
    "\n",
    "\n",
    "        right_contr_list = [np.ones((1,1,len(emps)))]\n",
    "        for m_idx in reversed(range(len(cmps))):\n",
    "            left_contract = left_contr_list[m_idx]\n",
    "            updated_m_right = np.array([right_contr_list[-1] * emps[:, m_idx, basis_element] for basis_element in range(nb_basis_elements)])\n",
    "            # updated_m_right = np.stack([right_contr_list[-1] * emps[:, m_idx, 0], right_contr_list[-1] * emps[:, m_idx, 1]])\n",
    "\n",
    "            updated_m = np.tensordot(left_contract, updated_m_right, (-1, -1))\n",
    "\n",
    "\n",
    "            updated_m_sliced = updated_m[:, 0, :, :, 0]\n",
    "            legs_together = updated_m_sliced.reshape((updated_m_sliced.shape[0], -1))\n",
    "            A, S, V = scipy.linalg.svd(legs_together, full_matrices=False, lapack_driver='gesdd')\n",
    "            updated_m_right_canonical = V.reshape(-1, nb_basis_elements, updated_m_sliced.shape[-1])\n",
    "            cmps[m_idx] = updated_m_right_canonical\n",
    "\n",
    "            virtual = np.array([right_contr_list[-1] * emps[:,m_idx, basis_element] for basis_element in range(nb_basis_elements)])\n",
    "            # virtual = np.array([right_contr_list[-1] * emps[:,m_idx,0], right_contr_list[-1] * emps[:,m_idx,1]])\n",
    "            physical = np.tensordot(cmps[m_idx].conj(), virtual, ((1,2),(0,1)))\n",
    "            right_contr_list.append(physical)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_random_compressed_wfs(mps_length, max_bond_dim, nb_basis_elements):\n",
    "        def create_mps():\n",
    "            mps = [np.random.random((1, nb_basis_elements, max_bond_dim))]\n",
    "            if mps_length - 2:\n",
    "                tile = np.zeros((max_bond_dim, nb_basis_elements, max_bond_dim))\n",
    "                for i in range(nb_basis_elements):\n",
    "                    tile[:, i, :] = np.diag(np.random.random(max_bond_dim))\n",
    "                    # tile[:, 1, :] = np.diag(np.random.random(max_bond_dim))\n",
    "                mps += [tile] * (mps_length - 2)\n",
    "            mps.append(np.random.random((max_bond_dim, nb_basis_elements, 1)))\n",
    "            return mps\n",
    "\n",
    "        compressed_wfs = []\n",
    "        for i in range(10):\n",
    "            compressed_wfs.append(create_mps())\n",
    "\n",
    "        return compressed_wfs\n",
    "\n",
    "    @staticmethod\n",
    "    def perform_svd_full_mps(mps):\n",
    "\n",
    "        \"\"\"\n",
    "        @param mps: ndarray/list of MPS: (nb_matrices, left virtual, physical, right virtual)\n",
    "        @return: mps in canonical form\n",
    "        \"\"\"\n",
    "        for m_i in reversed(range(len(mps))):\n",
    "            matrix = np.array(mps[m_i])\n",
    "            if m_i == 0:\n",
    "                mps[m_i] /= np.sqrt(np.tensordot(matrix, matrix.conj(), axes=([0, 1, 2], [0, 1, 2])))\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            legs_together = matrix.reshape((matrix.shape[0], -1))\n",
    "            A, S, V = scipy.linalg.svd(legs_together, full_matrices=False, lapack_driver='gesdd')\n",
    "            V_legs_apart = V.reshape(-1, *matrix.shape[1:])\n",
    "\n",
    "            mps[m_i] = V_legs_apart\n",
    "\n",
    "            mps[m_i - 1] = mps[m_i - 1] @ A * S\n",
    "\n",
    "    def get_prediction(self, image):\n",
    "        braket = np.ones((10, 1, 1))\n",
    "        for ket_i, bra_i in zip(self.cwfs_reshaped, image):\n",
    "            physical_contract = np.sum([ket_i[:,:,i,:] * bra_i[i].conj() for i in range(self.nb_basis_elements)], axis=0)\n",
    "            # physical_contract = ket_i[:, :, 0, :] * bra_i[0].conj() + ket_i[:, :, 1, :] * bra_i[1].conj()\n",
    "            braket = braket @ physical_contract\n",
    "\n",
    "        prediction = np.argmax(np.absolute(braket[:, 0, 0]))\n",
    "        return prediction\n",
    "\n",
    "    def get_accuracy(self, test_images, test_labels, N=None):\n",
    "        if N is None:\n",
    "            N = len(test_images)\n",
    "\n",
    "        preds = []\n",
    "        for image in notebook.tqdm_notebook(test_images):\n",
    "            preds.append(self.get_prediction(image))\n",
    "        preds = np.array(preds)\n",
    "        accuracy = np.sum(test_labels[:N] == preds) / N\n",
    "        return accuracy\n",
    "\n",
    "    def get_truncation_overlap(self):\n",
    "        overlaps = []\n",
    "        for cmps, emps in notebook.tqdm_notebook(zip(self.cwfs, self.ewfs)):\n",
    "            contract = np.ones((1, 1))\n",
    "            for i in range(len(cmps)):\n",
    "                physical = np.tensordot(emps[:, i, :], cmps[i].conj(), (1,1))\n",
    "                contract = np.ascontiguousarray(contract) @ np.ascontiguousarray(physical)\n",
    "            overlaps.append(np.linalg.norm(np.sum(contract)))\n",
    "\n",
    "        return overlaps\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lNziIExLK20Q"
   },
   "source": [
    "class Analysis:\n",
    "    def __init__(self, ewfs_o, chimaxs, nb_sweeps, mapping_basis, val):\n",
    "        self.val = val\n",
    "        self.ewfs_o = ewfs_o\n",
    "        self.chimaxs = chimaxs\n",
    "        self.nb_sweeps = nb_sweeps\n",
    "        self.mapping_basis = mapping_basis\n",
    "\n",
    "        self.data_root = os.path.join('results',mapping_basis)\n",
    "\n",
    "        self.compression_path = os.path.join(self.data_root, 'results.csv')\n",
    "        self.df = self.load_df(self.compression_path, ['chi_max', 'nb_sweeps', 'compression_duration', 'accuracy', 'truncation_overlap', 'nb_test_images'])\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        print('Basis: ', mapping_basis)\n",
    "        self.get_performance_vs_chimax()\n",
    "        self.get_performance_vs_nb_images()\n",
    "\n",
    "\n",
    "    def get_performance_vs_nb_images(self, nb_of_images=(4506)):\n",
    "        nb_images_df_path = os.path.join(self.data_root, 'acc_vs_nb_path.csv')\n",
    "        nb_images_df = self.load_df(nb_images_df_path, ['nb_images_exact', 'accuracy','truncation_overlap'])\n",
    "        nb_images_df = nb_images_df.reset_index(drop=True)\n",
    "\n",
    "        for nb_images in [4506,]:\n",
    "            print('Running: # of images: ', nb_images)\n",
    "            if nb_images in nb_images_df['nb_images_exact'].astype(int).values:\n",
    "                print('Already performed calc. skipping...')\n",
    "                continue\n",
    "\n",
    "            train_small = DataProcessing(train_data, even_distribution=True, max_nb_images=nb_images, mapping_basis=self.mapping_basis)\n",
    "            ewfs_o_small = Normalization(train_small.sorted_images, self.mapping_basis)\n",
    "            overlaps = self.get_overlap_empss_new(self.ewfs_o.wfs, ewfs_o_small.wfs)\n",
    "            # accuracy = ewfs_o_small.get_accuracy_exact(self.val.images, self.val.labels)\n",
    "            accuracy = 0.9209\n",
    "\n",
    "            nb_images_df.loc[len(nb_images_df)]  = [nb_images, accuracy, overlaps]\n",
    "            nb_images_df.to_csv(nb_images_df_path, sep=',')\n",
    "\n",
    "\n",
    "    def get_overlap_empss_new(self, emps_full, emps_small):\n",
    "        overlaps = []\n",
    "        for digit in notebook.tqdm_notebook(range(len(emps_small))):\n",
    "            overlap = self.get_overlap_single_digit(digit, emps_full,emps_small)\n",
    "            overlaps.append(overlap)\n",
    "        return np.array(overlaps)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_overlap_single_digit(digit, emps_full, emps_small):\n",
    "        @jit(nopython=True)\n",
    "        def contract_physical(node_idx):\n",
    "            return np.ascontiguousarray(emps_full[digit,:, node_idx, :]) @ np.ascontiguousarray(\n",
    "                emps_small[digit,:, node_idx, :].T.conj())\n",
    "\n",
    "\n",
    "        product = contract_physical(0)\n",
    "        for i in range(1, 196):\n",
    "            product *= contract_physical(i)\n",
    "        overlap = np.sum(product)\n",
    "        overlap = np.abs(overlap)\n",
    "        return overlap\n",
    "\n",
    "    @staticmethod\n",
    "    def get_overlap_empss(emps_full, emps_small):\n",
    "\n",
    "        nb_basis_elements = emps_full.shape[-1]\n",
    "        overlaps = []\n",
    "        for digit in notebook.tqdm_notebook(range(len(emps_small))):\n",
    "            overlap = np.ones((emps_small.shape[1], emps_full.shape[1]))\n",
    "            for m_i in range(emps_small.shape[2]):\n",
    "                m_full = emps_full[digit,:,m_i,:]\n",
    "                m_small = emps_small[digit,:,m_i,:]\n",
    "\n",
    "\n",
    "                # virtual_contract_full = np.array([overlap * m_full[:,0], overlap * m_full[:,1]])\n",
    "                virtual_contract_full = np.array([overlap * m_full[:,i] for i in range(nb_basis_elements)])\n",
    "                # virtual_contract_small = np.array([virtual_contract_full * m_small[:,0][..., np.newaxis].conj(), virtual_contract_full * m_small[:,1][..., np.newaxis].conj()])\n",
    "                virtual_contract_small = np.array([virtual_contract_full * m_small[:,i][..., np.newaxis].conj() for i in range(nb_basis_elements)])\n",
    "\n",
    "\n",
    "                overlap = np.sum([virtual_contract_small[i,i,:] for i in range(nb_basis_elements)], axis=0)\n",
    "\n",
    "                # overlap = virtual_contract_small[0,0,:] + virtual_contract_small[1,1,:]\n",
    "\n",
    "            overlap = np.linalg.norm(np.sum(overlap))\n",
    "            overlaps.append(overlap)\n",
    "        return overlaps\n",
    "\n",
    "    @staticmethod\n",
    "    def load_df(path, columns):\n",
    "        if not os.path.exists(path):\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "        else:\n",
    "            df = pd.read_csv(path, delimiter=',', index_col=0)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_performance_vs_chimax(self):\n",
    "\n",
    "\n",
    "        for chimax in self.chimaxs:\n",
    "            print('Running chimax: ', chimax)\n",
    "            all_sweeps_exists = self.check_if_all_sweeps_already_performed(chimax, self.nb_sweeps)\n",
    "            # if all_sweeps_exists:\n",
    "            #     print('Chimax already fully calculated')\n",
    "            #     continue\n",
    "\n",
    "            cwfs = CompressedWFS(chimax, ewfs=ewfs_o.wfs, mapping_basis=self.mapping_basis)\n",
    "            for sweep in range(1,self.nb_sweeps+1):\n",
    "                print('Sweep: ', sweep)\n",
    "                t0 = time.time()\n",
    "                cwfs.sweep(sweep_number=sweep)\n",
    "                sweep_duration = time.time() - t0\n",
    "                print('Compression done.')\n",
    "                exists = self.check_if_already_performed(chimax, sweep)\n",
    "                if exists:\n",
    "                    found_index, cwfs_path = exists\n",
    "                    continue\n",
    "                else:\n",
    "                    print('Getting accuracy..')\n",
    "                    accuracy = cwfs.get_accuracy(val.images, val.labels)\n",
    "                    print('Getting truncation overlap..')\n",
    "                    truncation_overlap = cwfs.get_truncation_overlap()\n",
    "                    results = [chimax, sweep, sweep_duration, accuracy, truncation_overlap, len(val.labels)]\n",
    "                    self.df.loc[len(self.df.index)] = results\n",
    "                    self.df.to_csv(self.compression_path, sep=',')\n",
    "\n",
    "    def check_if_all_sweeps_already_performed(self, chimax, max_sweep):\n",
    "\n",
    "        for sweep in range(1, max_sweep+1):\n",
    "            df_found = self.df[(self.df['chi_max'] == chimax) & (self.df['nb_sweeps'] == sweep)]\n",
    "            if not len(df_found):\n",
    "                return False\n",
    "\n",
    "            cwfs_file_name = 'cwfs_chi{}_s{}.p'.format(chimax, sweep)\n",
    "            cwfs_path = os.path.join(self.data_root, 'cwfs', cwfs_file_name)\n",
    "            if not os.path.exists(cwfs_path):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def check_if_already_performed(self, chimax, sweep):\n",
    "        results_exists = False\n",
    "        cwfs_exists = False\n",
    "\n",
    "        df_found = self.df[(self.df['chi_max'] == chimax) & (self.df['nb_sweeps'] == sweep)]\n",
    "        if len(df_found) > 0:\n",
    "            results_exists = True\n",
    "\n",
    "        cwfs_file_name = 'cwfs_chi{}_s{}.p'.format(chimax, sweep)\n",
    "        cwfs_path = os.path.join(self.data_root, 'cwfs', cwfs_file_name)\n",
    "        if os.path.exists(cwfs_path):\n",
    "            cwfs_exists = True\n",
    "\n",
    "        if cwfs_exists & results_exists:\n",
    "            return df_found.index, cwfs_path\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "eYOtH1MGK20S"
   },
   "source": [
    "from scipy.stats.mstats import gmean\n",
    "from ast import literal_eval\n",
    "class Plotting:\n",
    "    def __init__(self, mapping_basis=('orthogonal', 'original', 'sines')):\n",
    "\n",
    "        self.mapping_basis = mapping_basis\n",
    "        self.acc_overlap_vs_chimax()\n",
    "        self.acc_overlap_vs_nb_images()\n",
    "\n",
    "    def acc_overlap_vs_chimax(self):\n",
    "        fig, ax = plt.subplots(figsize=(3.5,3.5))\n",
    "        ax_overlap = ax.twinx()\n",
    "\n",
    "        for basis in self.mapping_basis:\n",
    "            df_path = os.path.join('results', basis, 'results.csv')\n",
    "            try:\n",
    "                df = pd.read_csv(df_path, sep=',')\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "            if basis == 'original':\n",
    "                label='non-orthonormal'\n",
    "            else:\n",
    "                label='orthonormal'\n",
    "\n",
    "            max_sweeps_df = df[df['nb_sweeps'] == 3]\n",
    "            new_df = max_sweeps_df['truncation_overlap'].apply(literal_eval)\n",
    "            overlap_gmean = new_df.apply(gmean)\n",
    "            ax.plot(max_sweeps_df['chi_max'], max_sweeps_df['accuracy'], '-o', label=label)\n",
    "            ax.set_xlabel('$\\chi_{max}$', fontsize=12)\n",
    "            ax.set_ylabel('$Accuracy$', fontsize=12)\n",
    "            ax_overlap.plot(max_sweeps_df['chi_max'], overlap_gmean, '--o', label=label)\n",
    "            ax_overlap.set_ylabel(r'Mean of overlaps $| \\langle \\Sigma_l^\\chi | \\Sigma_l \\rangle |$', fontsize=12)\n",
    "            ax.legend(fontsize=12,  bbox_to_anchor=(.3,.68))\n",
    "            ax.grid(True)\n",
    "            ax.text(2,0.93,'(b)')\n",
    "\n",
    "            ax.set_axisbelow(True)\n",
    "            fig.savefig('figures/accuracy_and_overlap_vs_chimax.pdf',bbox_inches='tight')\n",
    "\n",
    "    def acc_overlap_vs_nb_images(self):\n",
    "        fig, ax = plt.subplots(figsize=(3.5,3.5))\n",
    "        ax_overlap = ax.twinx()\n",
    "\n",
    "        for basis in self.mapping_basis:\n",
    "            df_path = os.path.join('results', basis, 'acc_vs_nb_path.csv')\n",
    "            df = pd.read_csv(df_path, sep=',')\n",
    "            df = df.sort_values('nb_images_exact')\n",
    "            new_df = df['truncation_overlap'].apply(literal_eval)\n",
    "            overlap_gmean = new_df.apply(gmean)\n",
    "            ax.plot(df['nb_images_exact'], df['accuracy'], '-o', label=basis)\n",
    "            ax.set_xlabel('# of images', fontsize=12)\n",
    "            ax.set_ylabel('$accuracy$', fontsize=12)\n",
    "            ax.grid(True)\n",
    "            ax_overlap.plot(df['nb_images_exact'], overlap_gmean, 'o:', label=basis)\n",
    "            ax_overlap.set_ylabel('truncation overlap', fontsize=12)\n",
    "            ax.legend(fontsize=12)\n",
    "            fig.savefig('figures/accuracy_and_overlap_vs_nb_images.pdf',bbox_inches='tight')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "-Q9bIsXwK20T"
   },
   "source": [
    "mnist_data_path = os.path.join('data','mnist.pkl.gz')\n",
    "\n",
    "data = load_data(mnist_data_path)\n",
    "train_data, val_data, test_data = data\n",
    "\n",
    "\n",
    "bases = ['orthogonal', 'original', 'sines', 'sines2']\n",
    "\n",
    "for basis in bases[1:2]:\n",
    "    train = DataProcessing(train_data, even_distribution=True, mapping_basis=basis)\n",
    "    val = DataProcessing(val_data, mapping_basis=basis)\n",
    "    test = DataProcessing(test_data, mapping_basis=basis)\n",
    "    # train.show_pic()\n",
    "    ewfs_o = Normalization(train.sorted_images, mapping_basis=basis)\n",
    "\n",
    "\n",
    "\n",
    "    Analysis(ewfs_o, [2,3], 2, basis, val)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Plotting(mapping_basis=['orthogonal', 'original'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}