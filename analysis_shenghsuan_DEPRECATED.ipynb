{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse.linalg\n",
    "from numba import jit\n",
    "from tqdm import notebook\n",
    "import pandas as pd\n",
    "\n",
    "from data_loader import *\n",
    "import os\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    def inner(*args, **kwargs):\n",
    "        t0 = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        print('duration of \"{}\": {}'.format(func.__name__, time.time() - t0))\n",
    "        return result\n",
    "\n",
    "    return inner\n",
    "\n",
    "\n",
    "def save_data_to_pickle(func, arguments, file_path, run_always=False):\n",
    "    if os.path.exists(file_path) and not run_always:\n",
    "\n",
    "        with open(file_path, 'rb') as f:\n",
    "            print('Data already exists, loading data...')\n",
    "            result = pickle.load(f)\n",
    "\n",
    "    else:\n",
    "        if not os.path.exists(os.path.dirname(file_path)):\n",
    "            print(os.path.dirname(file_path))\n",
    "            os.makedirs(os.path.dirname(file_path))\n",
    "        result = func(*arguments)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(result, f)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "class DataProcessing:\n",
    "    def __init__(self, dataset, mapping_basis, even_distribution=False, max_nb_images=None):\n",
    "        assert mapping_basis in ['orthogonal', 'original', 'sin_cos', 'sines', 'sines2'], 'mapping_basis should be one of [\"orthogonal\", \"original\", \"sin_cos\"]'\n",
    "        self.mapping_basis = mapping_basis\n",
    "        self.max_nb_images = max_nb_images\n",
    "        self.original_images, self.labels = dataset\n",
    "\n",
    "        self.images = self.transform(self.original_images)\n",
    "\n",
    "        if even_distribution:\n",
    "            self.sorted_images = self.get_digits(self.images, self.labels)\n",
    "            self.sorted_images = self.create_even_distribution(self.sorted_images)\n",
    "\n",
    "    @staticmethod\n",
    "    def flat_to_2d(batch):\n",
    "        H = W = int(np.sqrt(batch.shape[-1]))\n",
    "        return batch.reshape((-1, H, W))\n",
    "\n",
    "    @staticmethod\n",
    "    def square_to_flat(batch):\n",
    "        H = batch.shape[-1]\n",
    "        return batch.reshape((-1, H ** 2))\n",
    "\n",
    "    def mean_pooling(self, images_, filter_size):\n",
    "        images_2d = self.flat_to_2d(images_)\n",
    "        H, W = images_2d.shape[-2:]\n",
    "        H_new = H // filter_size\n",
    "        W_new = W // filter_size\n",
    "        new_shape = images_2d.shape[:1] + (H_new, filter_size, W_new, filter_size)\n",
    "        result = np.mean(images_2d.reshape(new_shape), axis=(2, 4))\n",
    "        result = self.square_to_flat(result)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def scale(images_, domain=(0, 255), range_=(0, 1)):\n",
    "        scaled = images_ / (domain[1] - domain[0]) * (range_[1] - range_[0]) + range_[0]\n",
    "        return scaled\n",
    "\n",
    "    def mapping(self, mat):\n",
    "        if self.mapping_basis == 'orthogonal':\n",
    "            qstate = np.array([np.exp(mat * 3.j * np.pi / 2) * np.cos(mat * np.pi / 2), np.exp(-mat * 3.j * np.pi / 2) * np.sin(mat * np.pi / 2)])\n",
    "        elif self.mapping_basis == 'original':\n",
    "            qstate = np.array([np.cos(mat * np.pi / 2), np.sin(mat * np.pi / 2)])\n",
    "        elif self.mapping_basis == 'sin_cos':\n",
    "            qstate = np.array([np.sin(np.pi*1/2 * mat),\n",
    "                               np.cos(np.pi*1/2 * mat),\n",
    "                               np.sin(np.pi*3/2 * mat),\n",
    "                               np.cos(np.pi*3/2 * mat)])\n",
    "\n",
    "        elif self.mapping_basis == 'sines':\n",
    "            qstate = np.array([np.sin(np.pi*1/2 * mat),\n",
    "                               np.sin(np.pi*3/2 * mat),\n",
    "                               np.sin(np.pi*5/2 * mat),\n",
    "                               np.sin(np.pi*7/2 * mat)])\n",
    "            qstate /= np.linalg.norm(qstate, axis=0)\n",
    "\n",
    "        elif self.mapping_basis == 'sines2':\n",
    "            qstate = np.array([np.sin(np.pi*1 * mat),\n",
    "                               np.sin(np.pi*2 * mat),\n",
    "                               np.sin(np.pi*3 * mat),\n",
    "                               np.sin(np.pi*4 * mat)])\n",
    "\n",
    "            # print(np.linalg.norm(qstate, axis=0).shape)\n",
    "            qstate /= (np.linalg.norm(qstate, axis=0))\n",
    "        else:\n",
    "            print('Mapping basis not valid')\n",
    "            return\n",
    "        return qstate.transpose(1, 2, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_digits(mat, labels_):\n",
    "        sorted_digits_ = []\n",
    "        for label in np.unique(labels_):\n",
    "            label_slice = (labels_ == label)\n",
    "            sorted_digits_.append(mat[label_slice])\n",
    "\n",
    "        return sorted_digits_\n",
    "\n",
    "    def transform(self, images_):\n",
    "        course_grain = self.mean_pooling(images_, 2)\n",
    "\n",
    "        if self.mapping_basis == 'sines':\n",
    "            scaled = self.scale(course_grain, domain=(0, 1), range_=(0.1,1))\n",
    "        elif self.mapping_basis == 'sines2':\n",
    "            scaled = self.scale(course_grain, domain=(0, 1), range_=(0.1,0.9))\n",
    "\n",
    "        else:\n",
    "            scaled = self.scale(course_grain, domain=(0, 1))\n",
    "        mapped = self.mapping(scaled)\n",
    "        return mapped\n",
    "\n",
    "    def create_even_distribution(self, batch):\n",
    "        if self.max_nb_images is None:\n",
    "            smallest_batch_size = np.min([len(dbatch) for dbatch in batch])\n",
    "            even_batch = np.array([dbatch[:smallest_batch_size] for dbatch in batch]).copy()\n",
    "            return even_batch\n",
    "        else:\n",
    "            even_batch = np.array([dbatch[:self.max_nb_images] for dbatch in batch]).copy()\n",
    "            return even_batch\n",
    "\n",
    "\n",
    "    def show_pic(self):\n",
    "        pixels = self.sorted_images.shape[-2]\n",
    "        h = int(pixels**(1/2))\n",
    "\n",
    "        for i in range(4):\n",
    "            plt.figure()\n",
    "            image  = self.sorted_images[0,0,:,i].reshape((h,h))\n",
    "            plt.imshow(image)\n",
    "\n",
    "\n",
    "\n",
    "class Normalization:\n",
    "    def __init__(self, batch, mapping_basis):\n",
    "        nb_images = batch[0].shape[0]\n",
    "        norms_save_path = os.path.join('results',mapping_basis,'norms.p')\n",
    "        self.norms = save_data_to_pickle(self.get_norms_all_digits, (batch,), norms_save_path)\n",
    "        self.wfs = self.normalize_all_digits(batch, self.norms)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_all_digits(batch, norms):\n",
    "        for i in range(len(batch)):\n",
    "\n",
    "            batch[i] /= (norms[i] ** (1 / (2 * batch[i].shape[1])))\n",
    "        return batch\n",
    "\n",
    "    def get_norms_all_digits(self, batch):\n",
    "        norms = []\n",
    "        for digit_batch in notebook.tqdm_notebook(batch):\n",
    "            norm = self.get_norm_single_digit(digit_batch)\n",
    "            norms.append(norm)\n",
    "        return np.array(norms)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_norm_single_digit(digit_batch):\n",
    "        @jit(nopython=True)\n",
    "        def contract_physical(node_idx):\n",
    "            return np.ascontiguousarray(digit_batch[:, node_idx, :]) @ np.ascontiguousarray(\n",
    "                digit_batch[:, node_idx, :].T.conj())\n",
    "\n",
    "\n",
    "        product = contract_physical(0)\n",
    "        for i in range(1, 196):\n",
    "            product *= contract_physical(i)\n",
    "        norm = np.sum(product)\n",
    "        norm = np.abs(norm)\n",
    "        print(norm)\n",
    "        return norm\n",
    "\n",
    "    @staticmethod\n",
    "    def get_overlaps_single_image(image_mps, dfunc):\n",
    "        physical_contract = np.sum([image_mps[:,i].conj() * dfunc[:,:,:,i] for i in range(image_mps.shape[-1])], axis=0)\n",
    "        virtual_contract = np.prod(physical_contract, 2)\n",
    "        overlap = np.sum(virtual_contract, 1)\n",
    "        overlap = np.abs(overlap)\n",
    "        return overlap\n",
    "\n",
    "    @timeit\n",
    "    def get_accuracy_exact(self, test_images, test_labels, N=None):\n",
    "        if N is None:\n",
    "            N = len(test_images)\n",
    "        preds = []\n",
    "        for image in notebook.tqdm_notebook(test_images[:N]):\n",
    "            pred = np.argmax(self.get_overlaps_single_image(image, self.wfs))\n",
    "            preds.append(pred)\n",
    "        preds = np.array(preds)\n",
    "        accuracy = np.sum(test_labels[:N] == preds) / N\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "class CompressedWFS:\n",
    "    def __init__(self, ewfs, compression_path):\n",
    "        self.ewfs = ewfs\n",
    "\n",
    "\n",
    "        self.nb_basis_elements = self.ewfs[0].shape[-1]\n",
    "\n",
    "\n",
    "        with open(compression_path, 'rb') as f:\n",
    "            self.cwfs = list(pickle.load(f).values())\n",
    "        self.cwfs_reshaped = self.reshape_cwfs()\n",
    "\n",
    "    def get_prediction(self, image):\n",
    "        braket = np.ones((10, 1, 1))\n",
    "        for ket_i, bra_i in zip(self.cwfs_reshaped, image):\n",
    "            physical_contract = np.sum([ket_i[:,:,i,:] * bra_i[i] for i in range(self.nb_basis_elements)], axis=0)\n",
    "            # physical_contract = ket_i[:, :, 0, :] * bra_i[0].conj() + ket_i[:, :, 1, :] * bra_i[1].conj()\n",
    "            braket = braket @ physical_contract\n",
    "\n",
    "        prediction = np.argmax(np.absolute(braket[:, 0, 0]))\n",
    "        return prediction\n",
    "\n",
    "    def get_accuracy(self, test_images, test_labels, N=None):\n",
    "        if N is None:\n",
    "            N = len(test_images)\n",
    "\n",
    "        preds = []\n",
    "        for image in notebook.tqdm_notebook(test_images):\n",
    "            preds.append(self.get_prediction(image))\n",
    "        preds = np.array(preds)\n",
    "        accuracy = np.sum(test_labels[:N] == preds) / N\n",
    "        return accuracy\n",
    "\n",
    "    def get_truncation_overlap(self):\n",
    "        overlaps = []\n",
    "        for cmps, emps in notebook.tqdm_notebook(zip(self.cwfs, self.ewfs)):\n",
    "            contract = np.ones((1, 1))\n",
    "            for i in range(len(cmps)):\n",
    "                physical = np.tensordot(emps[:, i, :], cmps[i].conj(), (1,1))\n",
    "                contract = np.ascontiguousarray(contract) @ np.ascontiguousarray(physical)\n",
    "            overlaps.append(np.linalg.norm(np.sum(contract)))\n",
    "\n",
    "        return overlaps\n",
    "\n",
    "    def reshape_cwfs(self):\n",
    "        cwfs_reshaped = []\n",
    "        for m_i in range(len(self.cwfs[0])):\n",
    "            single_m = []\n",
    "            for mps in self.cwfs:\n",
    "                single_m.append(mps[m_i])\n",
    "                print(mps[m_i].shape)\n",
    "            single_m = np.array(single_m)\n",
    "            cwfs_reshaped.append(single_m)\n",
    "        return cwfs_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Analysis:\n",
    "    def __init__(self, ewfs_o, val):\n",
    "\n",
    "        self.ewfs_o = ewfs_o\n",
    "        self.val = val\n",
    "\n",
    "        self.data_root = os.path.join('data','ShengHsuan-MPS-result')\n",
    "\n",
    "        self.compression_path = os.path.join(self.data_root, 'results.csv')\n",
    "        self.df = self.load_df(self.compression_path, ['chi_max', 'nb_sweeps', 'compression_duration', 'accuracy', 'truncation_overlap', 'nb_test_images'])\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        print('Basis: ', 'ShengHsuan')\n",
    "        self.get_performance_vs_chimax()\n",
    "\n",
    "\n",
    "        # self.get_performance_vs_nb_images()\n",
    "\n",
    "\n",
    "    def get_performance_vs_nb_images(self, nb_of_images=(4506)):\n",
    "        nb_images_df_path = os.path.join(self.data_root, 'acc_vs_nb_path.csv')\n",
    "        nb_images_df = self.load_df(nb_images_df_path, ['nb_images_exact', 'accuracy','truncation_overlap'])\n",
    "        nb_images_df = nb_images_df.reset_index(drop=True)\n",
    "\n",
    "        for nb_images in [4506,]:\n",
    "            print('Running: # of images: ', nb_images)\n",
    "            if nb_images in nb_images_df['nb_images_exact'].astype(int).values:\n",
    "                print('Already performed calc. skipping...')\n",
    "                continue\n",
    "\n",
    "            train_small = DataProcessing(train_data, even_distribution=True, max_nb_images=nb_images, mapping_basis=self.mapping_basis)\n",
    "            ewfs_o_small = Normalization(train_small.sorted_images, self.mapping_basis)\n",
    "            overlaps = self.get_overlap_empss_new(self.ewfs_o.wfs, ewfs_o_small.wfs)\n",
    "            accuracy = ewfs_o_small.get_accuracy_exact(self.val.images, self.val.labels)\n",
    "\n",
    "            nb_images_df.loc[len(nb_images_df)]  = [nb_images, accuracy, overlaps]\n",
    "            nb_images_df.to_csv(nb_images_df_path, sep=',')\n",
    "\n",
    "\n",
    "    def get_overlap_empss_new(self, emps_full, emps_small):\n",
    "        overlaps = []\n",
    "        for digit in notebook.tqdm_notebook(range(len(emps_small))):\n",
    "            overlap = self.get_overlap_single_digit(digit, emps_full,emps_small)\n",
    "            overlaps.append(overlap)\n",
    "        return np.array(overlaps)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_overlap_single_digit(digit, emps_full, emps_small):\n",
    "        @jit(nopython=True)\n",
    "        def contract_physical(node_idx):\n",
    "            return np.ascontiguousarray(emps_full[digit,:, node_idx, :]) @ np.ascontiguousarray(\n",
    "                emps_small[digit,:, node_idx, :].T.conj())\n",
    "\n",
    "\n",
    "        product = contract_physical(0)\n",
    "        for i in range(1, 196):\n",
    "            product *= contract_physical(i)\n",
    "        overlap = np.sum(product)\n",
    "        overlap = np.abs(overlap)\n",
    "        return overlap\n",
    "\n",
    "    @staticmethod\n",
    "    def get_overlap_empss(emps_full, emps_small):\n",
    "\n",
    "        nb_basis_elements = emps_full.shape[-1]\n",
    "        overlaps = []\n",
    "        for digit in notebook.tqdm_notebook(range(len(emps_small))):\n",
    "            overlap = np.ones((emps_small.shape[1], emps_full.shape[1]))\n",
    "            for m_i in range(emps_small.shape[2]):\n",
    "                m_full = emps_full[digit,:,m_i,:]\n",
    "                m_small = emps_small[digit,:,m_i,:]\n",
    "\n",
    "\n",
    "                # virtual_contract_full = np.array([overlap * m_full[:,0], overlap * m_full[:,1]])\n",
    "                virtual_contract_full = np.array([overlap * m_full[:,i] for i in range(nb_basis_elements)])\n",
    "                # virtual_contract_small = np.array([virtual_contract_full * m_small[:,0][..., np.newaxis].conj(), virtual_contract_full * m_small[:,1][..., np.newaxis].conj()])\n",
    "                virtual_contract_small = np.array([virtual_contract_full * m_small[:,i][..., np.newaxis].conj() for i in range(nb_basis_elements)])\n",
    "\n",
    "\n",
    "                overlap = np.sum([virtual_contract_small[i,i,:] for i in range(nb_basis_elements)], axis=0)\n",
    "\n",
    "                # overlap = virtual_contract_small[0,0,:] + virtual_contract_small[1,1,:]\n",
    "\n",
    "            overlap = np.linalg.norm(np.sum(overlap))\n",
    "            overlaps.append(overlap)\n",
    "        return overlaps\n",
    "\n",
    "    @staticmethod\n",
    "    def load_df(path, columns):\n",
    "        if not os.path.exists(path):\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "        else:\n",
    "            df = pd.read_csv(path, delimiter=',', index_col=0)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_performance_vs_chimax(self):\n",
    "\n",
    "        for file in glob.glob(os.path.join(self.data_root, '*.pkl')):\n",
    "            cwfs = CompressedWFS(self.ewfs_o.wfs, file)\n",
    "            accuracy = cwfs.get_accuracy(val.images, val.labels)\n",
    "            print('Getting truncation overlap..')\n",
    "            truncation_overlap = cwfs.get_truncation_overlap()\n",
    "            results = [chimax, sweep, sweep_duration, accuracy, truncation_overlap, len(val.labels)]\n",
    "            self.df.loc[len(self.df.index)] = results\n",
    "            self.df.to_csv(self.compression_path, sep=',')\n",
    "\n",
    "    def check_if_all_sweeps_already_performed(self, chimax, max_sweep):\n",
    "\n",
    "        for sweep in range(1, max_sweep+1):\n",
    "            df_found = self.df[(self.df['chi_max'] == chimax) & (self.df['nb_sweeps'] == sweep)]\n",
    "            if not len(df_found):\n",
    "                return False\n",
    "\n",
    "            cwfs_file_name = 'cwfs_chi{}_s{}.p'.format(chimax, sweep)\n",
    "            cwfs_path = os.path.join(self.data_root, 'cwfs', cwfs_file_name)\n",
    "            if not os.path.exists(cwfs_path):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def check_if_already_performed(self, chimax, sweep):\n",
    "        results_exists = False\n",
    "        cwfs_exists = False\n",
    "\n",
    "        df_found = self.df[(self.df['chi_max'] == chimax) & (self.df['nb_sweeps'] == sweep)]\n",
    "        if len(df_found) > 0:\n",
    "            results_exists = True\n",
    "\n",
    "        cwfs_file_name = 'cwfs_chi{}_s{}.p'.format(chimax, sweep)\n",
    "        cwfs_path = os.path.join(self.data_root, 'cwfs', cwfs_file_name)\n",
    "        if os.path.exists(cwfs_path):\n",
    "            cwfs_exists = True\n",
    "\n",
    "        if cwfs_exists & results_exists:\n",
    "            return df_found.index, cwfs_path\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "542bea780c834f46b23794d262ebf743"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5853.329\n",
      "333103.16\n",
      "4700.3496\n",
      "5148.478\n",
      "6581.214\n",
      "5401.6587\n",
      "7904.0054\n",
      "13512.609\n",
      "5186.15\n",
      "10674.089\n"
     ]
    }
   ],
   "source": [
    "mnist_data_path = os.path.join('data','mnist.pkl.gz')\n",
    "\n",
    "data = load_data(mnist_data_path)\n",
    "train_data, val_data, test_data = data\n",
    "\n",
    "basis='original'\n",
    "\n",
    "train = DataProcessing(train_data, even_distribution=True, mapping_basis=basis)\n",
    "val = DataProcessing(val_data, mapping_basis=basis)\n",
    "test = DataProcessing(test_data, mapping_basis=basis)\n",
    "# train.show_pic()\n",
    "ewfs_o = Normalization(train.sorted_images, mapping_basis=basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basis:  ShengHsuan\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.Analysis at 0x1db3e8b3490>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Analysis(ewfs_o=ewfs_o, val=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}